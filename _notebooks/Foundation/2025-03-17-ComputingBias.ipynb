{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title: Big Idea 5: Computing Bias\n",
    "permalink: /compbias/\n",
    "author: Avika, Gabi, Zoe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå What is Computing Bias?\n",
    "> **Bias:** An inclination or prejudice in favor of or against a person or a group of people, typically in a way that is unfair.\n",
    "\n",
    "Computing **bias** occurs when computer programs, algorithms, or systems produce results that unfairly favor or disadvantage certain groups. This bias can result from **biased data, flawed design, or unintended consequences** of programming.\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aeBLboArW8c?si=No1ZvRCvxdiEmbZB\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "\n",
    "---\n",
    "\n",
    "## üé• Example: Netflix Recommendation Bias\n",
    "Netflix provides content recommendations to users through algorithms. However, these algorithms can introduce bias in several ways:  \n",
    "\n",
    "### üîç **How Bias Can Occur:**\n",
    "- **Majority Preference Bias:**  \n",
    "  - Recommending mostly popular content, making it hard for less popular or niche content to be discovered.  \n",
    "- **Filtering Bias:**  \n",
    "  - Filtering out content that doesn‚Äôt fit a user‚Äôs perceived interests based on limited viewing history.  \n",
    "  - For example, if a user primarily watches romantic comedies, Netflix may avoid suggesting documentaries or foreign films, even if the user would enjoy them.  \n",
    "\n",
    "---\n",
    "\n",
    "# üßê How Does Computing Bias Happen?\n",
    "Computing bias can occur for various reasons, including:  \n",
    "\n",
    "### üìÇ **1. Unrepresentative or Incomplete Data:**  \n",
    "- Algorithms trained on data that **doesn't represent real-world diversity** will produce biased results.  \n",
    "\n",
    "### üìâ **2. Flawed or Biased Data:**  \n",
    "- Historical or existing prejudices reflected in the training data can lead to biased outputs.  \n",
    "\n",
    "### üìù **3. Data Collection & Labeling:**  \n",
    "- Human annotators may introduce biases due to different cultural or personal biases during the data labeling process.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Explicit Data vs. Implicit Data**\n",
    "\n",
    "### üìù **Explicit Data**\n",
    "**Definition:** Data that the user or programmer **directly provides**.\n",
    "\n",
    "- **Example:** On Netflix, users input personal information such as **name**, **age**, and **preferences**. They can also **rate shows** or **movies**.\n",
    "\n",
    "### üîç **Implicit Data**\n",
    "**Definition:** Data that is **inferred** from the user's actions or behavior, not directly provided.\n",
    "\n",
    "- **Example:** Netflix tracks your **viewing history**, **watch time**, and **interactions** with content. This data is then used to **recommend shows and movies** that Netflix thinks you might like.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è **Implications**\n",
    "- **Implicit Data** can lead to reinforcing **bias** by suggesting content based on **past behavior**, potentially **limiting diversity** and preventing users from discovering new genres.\n",
    "- **Explicit Data** is generally more **accurate** but can still be biased if **user input is limited** or influenced by the **design of the platform**.\n",
    "\n",
    "---\n",
    "## ü§î Popcorn Hack #1\n",
    "\n",
    "**What is an example of Explicit Data?**\n",
    "\n",
    "A) Netflix recommends shows based on your viewing history.  \n",
    "B) You provide your name, age, and preferences when creating a Netflix account.  \n",
    "C) Netflix tracks the time you spend watching certain genres.\n",
    "\n",
    "<div class=\"flip-container\">\n",
    "    <div class=\"flipper\">\n",
    "        <div class=\"front\">\n",
    "            <button class=\"button\" onclick=\"flipCard()\">Show Answer</button>\n",
    "        </div>\n",
    "        <div class=\"back\">\n",
    "            The answer is: B) You provide your name, age, and preferences when creating a Netflix account. This is an example of **explicit data**, as it is directly provided by the user.\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "    function flipCard() {\n",
    "        const flipContainer = event.target.closest('.flip-container');\n",
    "        flipContainer.classList.toggle('flipped');\n",
    "    }\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Types of Bias\n",
    "\n",
    "> **ü§ñ Algorithmic Bias**  \n",
    "- <ins>Algorithmic bias</ins> is bias generated from a repeatable but faulty **computer system** that produces inaccurate results.\n",
    "    - Example: A hiring algorithm at Amazon is trained on past employee data but the data shows that male candidates were hired more often than female candidates. Because of this, the system favored male candidates over female candidates because historical hiring practices were biased toward men.\n",
    "<div style=\"text-align: center; margin: 20px 0;\">\n",
    "    <img src=\"{{site.baseurl}}/images/algorithmic.jpg\" style=\"max-width: 80%; border-radius: 10px; box-shadow: 5px 5px 15px rgba(0,0,0,0.2);\">\n",
    "</div>\n",
    "> **üìà Data Bias**  \n",
    "- <ins>Data bias</ins> occurs when the data itself includes bias caused by **incomplete or erroneous information**.\n",
    "    - Example: A healthcare AI model predicts lower disease risk for certain populations. Since the AI model hasn't been introduced to other demographics, it would assume that data should include patients from a specific demographic, and not consider others.\n",
    "<div style=\"text-align: center; margin: 20px 0;\">\n",
    "    <img src=\"{{site.baseurl}}/images/data.png\" style=\"max-width: 80%; border-radius: 10px; box-shadow: 5px 5px 15px rgba(0,0,0,0.2);\">\n",
    "</div>\n",
    "\n",
    "> **üß† Cognitive Bias**  \n",
    "- <ins>Cognitive bias</ins> is when the person unintentionally introduces **their own bias** in the data.\n",
    "    - Example: A researcher conducting a study on social media usage unconsciously selects data that supports their belief that too much screen time leads to lower grades. This is a form of cognitive bias called confirmation bias because the researcher is searching for information to support their beliefs.\n",
    "<div style=\"text-align: center; margin: 20px 0;\">\n",
    "    <img src=\"{{site.baseurl}}/images/cognitive.jpg\" style=\"max-width: 80%; border-radius: 10px; box-shadow: 5px 5px 15px rgba(0,0,0,0.2);\">\n",
    "</div>\n",
    "---\n",
    "<style>\n",
    "    /* Style for the smaller button */\n",
    "    .button {\n",
    "        padding: 6px 12px;\n",
    "        font-size: 12px;\n",
    "        color: white;\n",
    "        background: linear-gradient(135deg, #6a89cc, #4a69bd); /* Blue gradient */\n",
    "        border: none;\n",
    "        border-radius: 6px;\n",
    "        cursor: pointer;\n",
    "        transition: background 0.3s, transform 0.2s, box-shadow 0.2s;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background: linear-gradient(135deg, #4a69bd, #1e3799); /* Darker blue gradient */\n",
    "        transform: scale(1.03);\n",
    "        box-shadow: 0 6px 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "\n",
    "    /* Container for the flip effect */\n",
    "    .flip-container {\n",
    "        perspective: 1000px;\n",
    "        margin: 20px 0;\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "    }\n",
    "\n",
    "    .flipper {\n",
    "        width: 100%;\n",
    "        height: 70px;\n",
    "        transform-style: preserve-3d;\n",
    "        transition: transform 0.6s;\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "    }\n",
    "\n",
    "    .front, .back {\n",
    "        position: absolute;\n",
    "        backface-visibility: hidden;\n",
    "        width: 100%;\n",
    "        height: 100%;\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        font-size: 16px;\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        padding: 15px;\n",
    "        border-radius: 8px;\n",
    "    }\n",
    "\n",
    "    .front {\n",
    "        background-color: #6a89cc;\n",
    "        color: white;\n",
    "    }\n",
    "\n",
    "    .back {\n",
    "        background-color: #4a69bd;\n",
    "        color: white;\n",
    "        transform: rotateY(180deg);\n",
    "    }\n",
    "\n",
    "    .flipped .flipper {\n",
    "        transform: rotateY(180deg);\n",
    "    }\n",
    "</style>\n",
    "\n",
    "## ü§î Popcorn Hack #2\n",
    "\n",
    "**What is an example of Data Bias?**  \n",
    "\n",
    "A) A hiring algorithm favors male candidates because the training data contains a disproportionate number of male resumes.  \n",
    "B) A system is trained on a dataset where certain groups, such as people with darker skin tones, are underrepresented.  \n",
    "C) A researcher intentionally selects data that supports their own beliefs about the impact of screen time on grades.  \n",
    "\n",
    "<div class=\"flip-container\">\n",
    "    <div class=\"flipper\">\n",
    "        <div class=\"front\">\n",
    "            <button class=\"button\" onclick=\"flipCard()\">Reveal Answer</button>\n",
    "        </div>\n",
    "        <div class=\"back\">\n",
    "            The answer is: B) A system is trained on a dataset where certain groups, such as people with darker skin tones, are underrepresented. This leads to the system performing poorly for these groups, which is an example of Data Bias.\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "    function flipCard() {\n",
    "        const flipContainer = event.target.closest('.flip-container');\n",
    "        flipContainer.classList.toggle('flipped');\n",
    "    }\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Style for the button */\n",
    "    .button {\n",
    "        padding: 6px 12px;\n",
    "        font-size: 12px;\n",
    "        color: white;\n",
    "        background: linear-gradient(135deg, #6a89cc, #4a69bd); /* Blue gradient */\n",
    "        border: none;\n",
    "        border-radius: 6px;\n",
    "        cursor: pointer;\n",
    "        transition: background 0.3s, transform 0.2s, box-shadow 0.2s;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background: linear-gradient(135deg, #4a69bd, #1e3799); /* Darker blue gradient */\n",
    "        transform: scale(1.03);\n",
    "        box-shadow: 0 6px 10px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "\n",
    "    /* Container for the flip effect */\n",
    "    .flip-container {\n",
    "        perspective: 1000px;\n",
    "        margin: 20px 0;\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "    }\n",
    "\n",
    "    .flipper {\n",
    "        width: 100%;\n",
    "        height: 70px;\n",
    "        transform-style: preserve-3d;\n",
    "        transition: transform 0.6s;\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "    }\n",
    "\n",
    "    .front, .back {\n",
    "        position: absolute;\n",
    "        backface-visibility: hidden;\n",
    "        width: 100%;\n",
    "        height: 100%;\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        font-size: 16px;\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        padding: 15px;\n",
    "        border-radius: 8px;\n",
    "    }\n",
    "\n",
    "    .front {\n",
    "        background-color: #6a89cc;\n",
    "        color: white;\n",
    "    }\n",
    "\n",
    "    .back {\n",
    "        background-color: #4a69bd;\n",
    "        color: white;\n",
    "        transform: rotateY(180deg);\n",
    "    }\n",
    "\n",
    "    .flipped .flipper {\n",
    "        transform: rotateY(180deg);\n",
    "    }\n",
    "\n",
    "    .question-container {\n",
    "        margin: 20px;\n",
    "        font-size: 18px;\n",
    "        color: #333;\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        line-height: 1.6;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "\n",
    "    /* Heading styling */\n",
    "    .heading {\n",
    "        font-size: 24px;\n",
    "        color: #333;\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "## Intentional Bias vs Unintentional Bias\n",
    "\n",
    "> **Intentional Bias:** The deliberate introduction of prejudice or unfairness into algorithms or systems, often by individuals or organizations, to achieve a specific outcome or advantage.  \n",
    "> \n",
    "> **Example:** A hiring algorithm designed to favor candidates from certain backgrounds by prioritizing certain keywords associated with privileged groups.\n",
    "\n",
    "Example: Imagine a company using a hiring algorithm to screen job applicants.\n",
    "\n",
    "<img src=\"{{site.baseurl}}/images/interviewer.webp\">\n",
    "- **Goal of the algorithm:** Select the most qualified candidates based on their resumes and experience.\n",
    "- However, the people who create this algorithm might intentionally include factors that are biased toward certain groups.\n",
    "\n",
    "For example, if the algorithm is designed to prioritize resumes with certain words or experiences that are more common among a specific gender or ethnic group, it might unfairly favor candidates from that group over others.\n",
    "\n",
    "<span>\n",
    "<img src=\"{{site.baseurl}}/images/resume.png\" width=\"500\" height=\"350\">\n",
    "<img src=\"{{site.baseurl}}/images/racism.jpg\" width=\"300\" height=\"350\">\n",
    "</span>\n",
    "<br>\n",
    "Also, if the algorithm gives extra weight to leadership positions in high-profile companies that are predominantly male or white, it may unintentionally (but intentionally by the developers) disadvantage women or people of color who have the same qualifications but worked in different environments.\n",
    "\n",
    "> **Unintentional Bias:** Occurs when algorithms, often trained on flawed or incomplete data, produce results that unfairly discriminate against certain groups.\n",
    "\n",
    "Example: A facial recognition software.\n",
    "- **Goal of the program:** Designed to identify people based on their facial features.\n",
    "- However, if the software is trained using a large dataset of photos primarily of one race, it can have trouble identifying individuals who look different.\n",
    "\n",
    "## Let's see a real-life example of this!\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/t4DT3tQqgRM?si=_9lYOfF6leUYKgug\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "\n",
    "For example, if the software is trained using pictures of people but the majority of those photos are of lighter-skinned individuals, the system may have trouble accurately recognizing people with darker skin tones.\n",
    "\n",
    "This unintentional bias happens because the developers didn‚Äôt purposefully choose to exclude people with darker skin, but because the dataset they used happened to be unbalanced.\n",
    "\n",
    "As a result, the system works better for lighter-skinned people and struggles with darker-skinned people, even though the goal is to treat everyone equally.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Popcorn Hack #3\n",
    "\n",
    "**What is an example of Unintentional Bias?**  \n",
    "A) A social media algorithm prioritizes content from a specific group of influencers because of their background.  \n",
    "B) A facial recognition system works better for lighter-skinned individuals because of an unbalanced dataset.  \n",
    "C) A hiring algorithm is designed to give preference to candidates from a specific ethnicity.  \n",
    "\n",
    "<div class=\"flip-container\">\n",
    "    <div class=\"flipper\">\n",
    "        <div class=\"front\">\n",
    "            <button class=\"button\" onclick=\"flipCard()\">Show Answer</button>\n",
    "        </div>\n",
    "        <div class=\"back\">\n",
    "            B) A facial recognition system works better for lighter-skinned individuals because of an unbalanced dataset. This is an example of unintentional bias, as the system was not purposefully designed to favor one group over another, but the dataset led to biased results.\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "    function flipCard() {\n",
    "        const flipContainer = event.target.closest('.flip-container');\n",
    "        flipContainer.classList.toggle('flipped');\n",
    "    }\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü **Mitigation Strategies**  \n",
    "\n",
    "Mitigation strategies aim to **prevent computing bias** by gathering and using more **diverse and representative data** throughout the algorithm's lifecycle.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîç **1. Pre-processing Phase (Model Planning & Preparation)**  \n",
    "- **Purpose:** Identify and fix issues in data collection to ensure accurate model training.  \n",
    "- **Actions:**  \n",
    "  - Managing missing data.  \n",
    "  - Ensuring data diversity.  \n",
    "  - Selecting relevant variables.  \n",
    "- ‚úÖ **Outcome:** Prevents biased data from being used to train the model.  \n",
    "\n",
    "---\n",
    "\n",
    "### üß© **2. In-processing Phase (Algorithm Development & Validation)**  \n",
    "- **Purpose:** Address biases during training and validation of AI algorithms.  \n",
    "- **Actions:**  \n",
    "  - Inserting synthetic samples representing minority cases.  \n",
    "  - Using cross-validation strategies.  \n",
    "- ‚úÖ **Outcome:** Promotes equal representation across demographics.  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **3. Post-processing Phase (Deployment & Usage)**  \n",
    "- **Purpose:** Implement the model and ensure fair application in real-world settings.  \n",
    "- **Actions:**  \n",
    "  - Monitoring model performance in deployment.  \n",
    "  - Adjusting outputs to reduce bias.  \n",
    "- ‚úÖ **Outcome:** Ensures the model functions fairly for all user groups.  \n",
    "\n",
    "---\n",
    "\n",
    "> **** \n",
    "- **Pre-processing Phase (Model Planning and Preparation)**\n",
    "    - Looking for and fixing any problems in the data collection process to ensure that the data is accurate for model training (management of missing data, ensure data diversity, select relevant variables, etc.).\n",
    "    - Prevents biased data from being used to train the model\n",
    "- **In-processing Phase (Algorithm Development & Validation)**\n",
    "    - In-processing represents all activities surrounding the training and validation phase of an AI algorithm (inserting synthetic sampls that are representative of minority class cases, using cross-validation strategies, etc.)\n",
    "    - Identifies biases or other vulnerabilities in the model and promote equal representation for all demographics\n",
    "- **Post-processing Phase (Clinical Deployment)** \n",
    "    - This phase encompasses a model‚Äôs implementation after it's been deployed and used in a live environment by others.\n",
    "    - It collects clinical data and interprets it to assess ensure compliance and refine future applications"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö **Computing Bias - Homework Questions**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Multiple-Choice Questions**  \n",
    "\n",
    "**1. What is computing bias?**  \n",
    "A. A technical error in hardware causing malfunctions.  \n",
    "B. A program or algorithm producing results that favor or disadvantage certain groups. \n",
    "C. A mistake in the code causing a program to crash.  \n",
    "D. The act of manually inputting incorrect data.  \n",
    "\n",
    "---\n",
    "\n",
    "**2. What is the primary cause of bias in computing systems?**  \n",
    "A. Poor internet connection.  \n",
    "B. Efficient programming techniques.  \n",
    "C. Increased processing power.  \n",
    "D. Unrepresentative or incomplete data used to train algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Which of the following is an example of implicit data collection?**  \n",
    "A. Manually selecting your preferred language on a streaming platform.  \n",
    "B. Filling out a survey about your favorite movies.  \n",
    "C. Netflix tracking your watch history and suggesting similar shows. \n",
    "D. Clicking ‚Äúlike‚Äù on a specific genre on Netflix.  \n",
    "\n",
    "---\n",
    "\n",
    "**4. What is a common issue when algorithms are trained on biased datasets?**  \n",
    "A. They can reinforce existing societal biases. \n",
    "B. They run faster.  \n",
    "C. They become more accurate.  \n",
    "D. They use less memory.  \n",
    "\n",
    "---\n",
    "\n",
    "**5. Which of the following could help reduce computing bias in recommendation systems?**  \n",
    "A. Ignoring user preferences.  \n",
    "B. Deleting all user data.  \n",
    "C. Using diverse and representative training data.\n",
    "D. Only using data from popular sources.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è **Short-Answer Question**  \n",
    "\n",
    "**Explain the difference between implicit and explicit data. Provide an example of each.**\n",
    "\n",
    "## üíØ **Scoring Rubric:**\n",
    "\n",
    "| Criteria                                  | Description                                                                       | Points |\n",
    "|-------------------------------------------|-----------------------------------------------------------------------------------|--------|\n",
    "| **Multiple-Choice Questions (0.5 points total)** | Each correct answer is worth 0.1 points.                             | 0.5    |\n",
    "| Question 1                                | Award 0.1 point if the correct option is selected.                 | 0.1    |\n",
    "| Question 2                                | Award 0.1 point if the correct option is selected.                 | 0.1    |\n",
    "| Question 3                                | Award 0.1 point if the correct option is selected.                 | 0.1    |\n",
    "| Question 4                                | Award 0.1 point if the correct option is selected.                 | 0.1    |\n",
    "| Question 5                                | Award 0.1 point if the correct option is selected.                 | 0.1    |\n",
    "| **Short-Answer Question (0.5 points total)** | Explanation of implicit vs. explicit data, with accurate examples. | 0.5    |\n",
    "| Clarity & Accuracy                        | Clear, concise, and correct explanation of implicit vs. explicit data. | 0.25   |\n",
    "| Examples Provided                         | Provides appropriate examples for both implicit and explicit data.  | 0.25   |\n",
    "| **Total**                                 |                                                                     | **1.0**  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
